# This workflow will build a new container weekly running the census data pipeline weekly pulls

name: Data Pipeline - Bucket 2 semiannually
on:
  #schedule:
    #- cron: "0 13 * * 1"
    #- cron: "*/1 * * * *"
  push:
    branches:
      - github-actions-data-pipeline
  workflow_dispatch:
env: 
  BUCKET_COMMAND: bucket_2_semiannually
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    environment: 
      name: Test
    steps:
      # checkout the repo
      - name: "Checkout GitHub Action"
        uses: actions/checkout@master
      
      - name: Log in with Azure
        uses: azure/login@v1
        with:
          creds: '${{ secrets.AZURE_CREDENTIALS }}'

      - name: Azure CLI script
        uses: azure/CLI@v1
        with:
          azcliversion: 2.0.72
          inlineScript: |
            az storage blob download \
             --account-name citpipelinedata \
             --container-name data \
             --name census_divisions.zip \
             --file census_divisions.zip \
             --account-key ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }} \
             --auth-mode key

      - name: "Generate env file"
        run: |
            echo 'POSTGRES_DB = ${{ secrets.POSTGRES_DB_TEST }}' >> .env 
            echo 'POSTGRES_DJANGO_USER = ${{ secrets.POSTGRES_DJANGO_USER_TEST }}' >> .env 
            echo 'POSTGRES_DJANGO_PASSWORD = ${{ secrets.POSTGRES_DJANGO_PASSWORD_TEST }}' >> .env 
            echo 'POSTGRES_HOST = ${{ secrets.POSTGRES_HOST_TEST }}' >> .env
            echo 'BUCKET_COMMAND = ${{ env.BUCKET_COMMAND }}' >> .env

      - name: Start containers
        run: |
            mkdir datafiles
            cp census_divisions.zip datafiles
            docker-compose -f cit-api/docker-compose.yml up --build